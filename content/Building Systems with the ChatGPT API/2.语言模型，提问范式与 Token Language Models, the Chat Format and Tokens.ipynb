{"cells":[{"cell_type":"markdown","id":"ae5bcee9-6588-4d29-bbb9-6fb351ef6630","metadata":{},"source":["# 第二章 语言模型，提问范式与 Token\n","\n"," - [一、环境配置](#一、环境配置)\n","     - [1.1 加载 API key 和一些 Python 的库。](#1.1-加载-API-key-和一些-Python-的库。)\n","     - [1.2 Helper function 辅助函数](#1.2-Helper-function-辅助函数)\n"," - [二、尝试向模型提问并得到结果](#二、尝试向模型提问并得到结果)\n"," - [三、Tokens](#三、Tokens)\n"," - [四、Helper function 辅助函数 (提问范式)](#四、Helper-function-辅助函数-(提问范式))\n"]},{"cell_type":"markdown","id":"baaf0c21","metadata":{},"source":["在本章中，我们将和您分享大型语言模型（LLM）的工作原理、训练方式以及分词器（tokenizer）等细节对 LLM 输出的影响。我们还将介绍 LLM 的提问范式（chat format），这是一种指定系统消息（system message）和用户消息（user message）的方式，让您了解如何利用这种能力。"]},{"cell_type":"markdown","id":"0c797991-8486-4d79-8c1d-5dc0c1289c2f","metadata":{},"source":["## 一、环境配置"]},{"cell_type":"markdown","id":"e33004b0","metadata":{},"source":["### 1.1 加载 API key 和一些 Python 的库。\n","在本课程中，为您提供了一些加载 OpenAI API key 的代码。"]},{"cell_type":"code","execution_count":31,"id":"fddf1a10","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.23.6)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.3.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (2.7.1)\n","Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai) (4.64.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai) (4.10.0)\n","Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n","Requirement already satisfied: langchain in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.1.16)\n","Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.0.29)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.0.34)\n","Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.1.46)\n","Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.0.1)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.1.51)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (1.24.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.7.1)\n","Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Requirement already satisfied: tiktoken in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.6.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tiktoken) (2024.4.16)\n","Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"]}],"source":["!pip install openai\n","!pip install langchain\n","!pip install --upgrade tiktoken"]},{"cell_type":"code","execution_count":32,"id":"19cd4e96","metadata":{"height":132},"outputs":[{"name":"stdout","output_type":"stream","text":["sk\n"]}],"source":["import os\n","from openai import OpenAI\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","client = OpenAI(\n","  api_key=os.environ['OPENAI_API_KEY']\n",")\n","deployment = \"gpt-3.5-turbo\"  # Typically, you would use this if specifying a particular model or deployment.\n","print(client.api_key[:2]) #  保证key安全，不要泄露"]},{"cell_type":"markdown","id":"47ba0938-7ca5-46c4-a9d1-b55708d4dc7c","metadata":{},"source":["### 1.2 Helper function 辅助函数\n","如果之前曾参加过《ChatGPT Prompt Engineering for Developers》课程，那么对此就相对较为熟悉。\n","调用该函数输入 Prompt 其将会给出对应的 Completion 。"]},{"cell_type":"code","execution_count":33,"id":"1ed96988","metadata":{"height":149},"outputs":[{"data":{"text/plain":["'Hello! How can I assist you today?'"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["def get_completion(prompt):\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = client.chat.completions.create(\n","        model=deployment,                                        \n","        messages=messages,\n","        temperature=0,  # this controls the randomness of the model's output\n","        max_tokens=1024\n","    )\n","    return response.choices[0].message.content\n","\n","# function test\n","get_completion(\"hello\")"]},{"cell_type":"markdown","id":"fe10a390-2461-447d-bf8b-8498db404c44","metadata":{},"source":["## 二、尝试向模型提问并得到结果"]},{"cell_type":"markdown","id":"50317bec","metadata":{},"source":["LLM 可以通过使用监督学习来构建，通过不断预测下一个词来学习。\n","并且，给定一个大的训练集，有数百亿甚至更多的词，你可以创建一个大规模的训练集，你可以从一\n","句话或一段文本的一部分开始，反复要求语言模型学习预测下一个词是什么"]},{"cell_type":"markdown","id":"325afca0","metadata":{},"source":["LLM 主要分为两种类型：基础语言模型（Base LLM）和越来越受欢迎的指令微调语言模型（Instruction Tuned LLM）。基础语言模型通过反复预测下一个词来训练，因此如果我们给它一个 Prompt，比如“从前有一只独角兽”，它可能通过逐词预测来完成一个关于独角兽在魔法森林中与其他独角兽朋友们生活的故事。\n","\n","然而，这种方法的缺点是，如果您给它一个 Prompt，比如“中国的首都是哪里？”，很可能它数据中有一段互联网上关于中国的测验问题列表。这时，它可能会用“中国最大的城市是什么？中国的人口是多少？”等等来回答这个问题。但实际上，您只是想知道中国的首都是什么，而不是列举所有这些问题。然而，指令微调语言模型会尝试遵循 Prompt，并给出“中国的首都是北京”的回答。\n","\n","那么，如何将基础语言模型转变为指令微调语言模型呢？这就是训练一个指令微调语言模型（例如ChatGPT）的过程。首先，您需要在大量数据上训练基础语言模型，因此需要数千亿个单词，甚至更多。这个过程在大型超级计算系统上可能需要数月时间。训练完基础语言模型后，您会通过在一小部分示例上进行进一步的训练，使模型的输出符合输入的指令。例如，您可以请承包商帮助您编写许多指令示例，并对这些指令的正确回答进行训练。这样就创建了一个用于微调的训练集，让模型学会在遵循指令的情况下预测下一个词是什么。\n","\n","之后，为了提高语言模型输出的质量，常见的方法是让人类对许多不同输出进行评级，例如是否有用、是否真实、是否无害等。然后，您可以进一步调整语言模型，增加生成高评级输出的概率。这通常使用强化学习中的人类反馈（RLHF）技术来实现。相较于训练基础语言模型可能需要数月的时间，从基础语言模型到指令微调语言模型的转变过程可能只需要数天时间，使用较小规模的数据集和计算资源。"]},{"cell_type":"code","execution_count":34,"id":"e1cc57b2","metadata":{"height":72},"outputs":[{"name":"stdout","output_type":"stream","text":["The capital of China is Beijing.\n"]}],"source":["response = get_completion(\"What is the capital of China?\")\n","print(response)"]},{"cell_type":"code","execution_count":35,"id":"10f34f3b","metadata":{"height":64},"outputs":[{"name":"stdout","output_type":"stream","text":["中国的首都是北京。\n"]}],"source":["response = get_completion(\"中国的首都是哪里？\")\n","print(response)"]},{"cell_type":"markdown","id":"b83d4e38-3e3c-4c5a-a949-040a27f29d63","metadata":{},"source":["## 三、Tokens"]},{"cell_type":"markdown","id":"76233527","metadata":{},"source":["到目前为止对 LLM 的描述中，我们将其描述为一次预测一个单词，但实际上还有一个更重要的技术细节。即 **`LLM 实际上并不是重复预测下一个单词，而是重复预测下一个 token`** 。当 LLM 接收到输入时，它将将其转换为一系列 token，其中每个 token 都代表常见的字符序列。例如，对于 \"Learning new things is fun!\" 这句话，每个单词都被转换为一个 token ，而对于较少使用的单词，如 \"Prompting as powerful developer tool\"，单词 \"prompting\" 会被拆分为三个 token，即\"prom\"、\"pt\"和\"ing\"。\n","\n","当您要求 ChatGPT 颠倒 \"lollipop\" 的字母时，由于分词器（tokenizer） 将 \"lollipop\" 分解为三个 token，即 \"l\"、\"oll\"、\"ipop\"，因此 ChatGPT 难以正确输出字母的顺序。您可以通过在字母之间添加连字符或空格的方式，使分词器将每个字母分解为单独的 token，从而帮助 ChatGPT 更好地认识单词中的每个字母并正确输出它们。"]},{"cell_type":"code","execution_count":36,"id":"cc2d9e40","metadata":{"height":64},"outputs":[{"name":"stdout","output_type":"stream","text":["pilpolol\n"]}],"source":["# 为了更好展示效果，这里就没有翻译成中文的 Prompt\n","# 注意这里的字母翻转出现了错误，吴恩达老师正是通过这个例子来解释 token 的计算方式\n","response = get_completion(\"Take the letters in lollipop \\\n","and reverse them\")\n","print(response)"]},{"cell_type":"markdown","id":"9d2b14d0-749d-4a79-9812-7b00ace9ae6f","metadata":{},"source":["\"lollipop\" in reverse should be \"popillol\""]},{"cell_type":"code","execution_count":37,"id":"37cab84f","metadata":{"height":88},"outputs":[{"name":"stdout","output_type":"stream","text":["p-o-p-i-l-l-o-l\n"]}],"source":["response = get_completion(\"\"\"Take the letters in \\\n","l-o-l-l-i-p-o-p and reverse them\"\"\")\n","\n","print(response)"]},{"cell_type":"markdown","id":"f5a6cb95","metadata":{},"source":["![Tokens.png](/docs/figures/C2/tokens.png)"]},{"cell_type":"markdown","id":"8b46bc72","metadata":{},"source":["对于英文输入，一个 token 一般对应 4 个字符或者四分之三个单词；对于中文输入，一个 token 一般对应一个或半个词。\n","\n","不同模型有不同的 token 限制，需要注意的是，这里的 token 限制是输入的 Prompt 和输出的 completion 的 token 数之和，因此输入的 Prompt 越长，能输出的 completion 的上限就越低。\n","\n","ChatGPT3.5-turbo 的 token 上限是 4096。"]},{"cell_type":"markdown","id":"c8b88940-d3ab-4c00-b5c0-31531deaacbd","metadata":{},"source":["## 四、Helper function 辅助函数 (提问范式)\n","下面是课程中用到的辅助函数。\n","下图是 OpenAI 提供的一种提问范式，接下来吴恩达老师就是在演示如何利用这种范式进行更好的提问\n","![Chat-format.png](../../figures/Chat-format.png)"]},{"cell_type":"markdown","id":"9e6b6b3d","metadata":{},"source":["System 信息用于指定模型的规则，例如设定、回答准则等，而 assistant 信息就是让模型完成的具体指令"]},{"cell_type":"code","execution_count":38,"id":"8f89efad","metadata":{"height":200},"outputs":[],"source":["def get_completion_from_messages(messages, \n","                                 model=\"gpt-3.5-turbo\", \n","                                 temperature=0, \n","                                 max_tokens=500):\n","    '''\n","    封装一个支持更多参数的自定义访问 OpenAI GPT3.5 的函数\n","\n","    参数: \n","    messages: 这是一个消息列表，每个消息都是一个字典，包含 role(角色）和 content(内容)。角色可以是'system'、'user' 或 'assistant’，内容是角色的消息。\n","    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，也可以选择 gpt-4\n","    temperature: 这决定模型输出的随机程度，默认为0，表示输出将比较确定。增加温度会使输出更随机。\n","    max_tokens: 这决定模型输出的最大的 token 数。\n","    '''\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature, # 这决定模型输出的随机程度\n","        max_tokens=max_tokens, # 这决定模型输出的最大的 token 数\n","    )\n","    content = response.choices[0].message.content\n","\n","\n","    return content"]},{"cell_type":"code","execution_count":39,"id":"b28c3424","metadata":{"height":183},"outputs":[{"name":"stdout","output_type":"stream","text":["Oh, the happy carrot so bright and so sweet,\n","In the garden it grows, where the sun and rain meet.\n","With a cheerful orange hue and a leafy green top,\n","This jolly little veggie will never stop!\n","It jumps and it bounces with joy each day,\n","The happiest carrot in every way!\n","So let's all smile and give a cheer,\n","For the joyous carrot, so full of good cheer!\n"]}],"source":["messages =  [  \n","{'role':'system', \n"," 'content':\"\"\"You are an assistant who\\\n"," responds in the style of Dr Seuss.\"\"\"},    \n","{'role':'user', \n"," 'content':\"\"\"write me a very short poem\\\n"," about a happy carrot\"\"\"},  \n","] \n","response = get_completion_from_messages(messages, temperature=1)\n","print(response)"]},{"cell_type":"code","execution_count":40,"id":"3d0ef08f","metadata":{"height":149},"outputs":[{"name":"stdout","output_type":"stream","text":["在大海深处有只小鲸鱼，\n","快乐自在像风儿舒舒。\n","它跃出水面欢歌笑，\n","尾巴翻飞有模有样。\n","\n","蓝蓝的海水是它的乐园，\n","游来游去从不懒惰。\n","和海豚一起追逐泡沫，\n","跳跃欢快展现活力无穷。\n","\n","快乐的小鲸鱼是海洋的宝，\n","带来笑声让人陶醉。\n","在涌动的浪花中自由自在，\n","快乐如歌是它的标签。\n"]}],"source":["messages =  [  \n","{'role':'system', \n"," 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n","{'role':'user', \n"," 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n","] \n","response = get_completion_from_messages(messages, temperature=1)\n","print(response)"]},{"cell_type":"code","execution_count":41,"id":"56c6978d","metadata":{"height":183},"outputs":[{"name":"stdout","output_type":"stream","text":["Once there was a cheerful carrot named Carl who loved making friends with all the veggies in the garden.\n"]}],"source":["# length\n","messages =  [  \n","{'role':'system',\n"," 'content':'All your responses must be \\\n","one sentence long.'},    \n","{'role':'user',\n"," 'content':'write me a story about a happy carrot'},  \n","] \n","response = get_completion_from_messages(messages, temperature =1)\n","print(response)"]},{"cell_type":"code","execution_count":42,"id":"e34c399e","metadata":{"height":166},"outputs":[{"name":"stdout","output_type":"stream","text":["小鲸鱼欢快地在海洋中游泳，与海洋的朋友们相处融洽，每天都过得充满快乐和幸福。\n"]}],"source":["# 长度控制\n","messages =  [  \n","{'role':'system',\n"," 'content':'你的所有答复只能是一句话'},    \n","{'role':'user',\n"," 'content':'写一个关于快乐的小鲸鱼的故事'},  \n","] \n","response = get_completion_from_messages(messages, temperature =1)\n","print(response)"]},{"cell_type":"code","execution_count":43,"id":"14fd6331","metadata":{"height":217},"outputs":[{"name":"stdout","output_type":"stream","text":["In a garden so bright, there lived a carrot named Sprite.\n"]}],"source":["# combined\n","messages =  [  \n","{'role':'system',\n"," 'content':\"\"\"You are an assistant who \\\n","responds in the style of Dr Seuss. \\\n","All your responses must be one sentence long.\"\"\"},    \n","{'role':'user',\n"," 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n","] \n","response = get_completion_from_messages(messages, \n","                                        temperature =1)\n","print(response)"]},{"cell_type":"code","execution_count":44,"id":"0ca678de","metadata":{"height":181},"outputs":[{"name":"stdout","output_type":"stream","text":["在大海深处有只小鲸鱼，欢快地跃出水面，快乐地歌唱，世界因它的存在而充满了欢乐与光芒。\n"]}],"source":["# 以上结合\n","messages =  [  \n","{'role':'system',\n"," 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答，只回答一句话'},    \n","{'role':'user',\n"," 'content':'写一个关于快乐的小鲸鱼的故事'},\n","] \n","response = get_completion_from_messages(messages, temperature =1)\n","print(response)"]},{"cell_type":"code","execution_count":45,"id":"89a70c79","metadata":{"height":370},"outputs":[],"source":["def get_completion_and_token_count(messages, \n","                                   model=\"gpt-3.5-turbo\", \n","                                   temperature=0, \n","                                   max_tokens=500):\n","    \"\"\"\n","    使用 OpenAI 的 GPT-3 模型生成聊天回复，并返回生成的回复内容以及使用的 token 数量。\n","\n","    参数:\n","    messages: 聊天消息列表。\n","    model: 使用的模型名称。默认为\"gpt-3.5-turbo\"。\n","    temperature: 控制生成回复的随机性。值越大，生成的回复越随机。默认为 0。\n","    max_tokens: 生成回复的最大 token 数量。默认为 500。\n","\n","    返回:\n","    content: 生成的回复内容。\n","    token_dict: 包含'prompt_tokens'、'completion_tokens'和'total_tokens'的字典，分别表示提示的 token 数量、生成的回复的 token 数量和总的 token 数量。\n","    \"\"\"\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature, # 这决定模型输出的随机程度\n","        max_tokens=max_tokens, # 这决定模型输出的最大的 token 数\n","    )\n","    content = response.choices[0].message.content\n","    #print(response)\n","    token_dict = {\n","        'prompt_tokens': response.usage.prompt_tokens,\n","        'completion_tokens': response.usage.completion_tokens,\n","        'total_tokens': response.usage.total_tokens,\n","    }\n","\n","    return content, token_dict"]},{"cell_type":"code","execution_count":46,"id":"a64cf3c6","metadata":{"height":166},"outputs":[{"name":"stdout","output_type":"stream","text":["Oh, the happy carrot, so bright and so bold,\n","In the garden, its story is joyfully told.\n","With a leafy green top and a vibrant orange hue,\n","It dances and sings, bringing smiles to you.\n"]}],"source":["messages = [\n","{'role':'system', \n"," 'content':\"\"\"You are an assistant who responds\\\n"," in the style of Dr Seuss.\"\"\"},    \n","{'role':'user',\n"," 'content':\"\"\"write me a very short poem \\ \n"," about a happy carrot\"\"\"},  \n","] \n","response, token_dict = get_completion_and_token_count(messages)\n","print(response)"]},{"cell_type":"code","execution_count":47,"id":"cfd8fbd4","metadata":{"height":146},"outputs":[{"name":"stdout","output_type":"stream","text":["在大海深处有一只小鲸鱼，\n","它快乐地游来游去，不停歇。\n","尾巴摆动，水花飞溅，\n","欢乐的歌声响彻海洋。\n","\n","它和海豚一起跳舞，\n","在阳光下闪闪发亮。\n","小鱼儿们围着它转圈，\n","快乐的时光总是那么短暂。\n","\n","小鲸鱼笑得灿烂如阳光，\n","它是大海中的快乐使者。\n","让我们跟随它的脚步，\n","一起享受生活的美好时刻。\n"]}],"source":["messages =  [  \n","{'role':'system', \n"," 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n","{'role':'user', \n"," 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n","] \n","response, token_dict = get_completion_and_token_count(messages)\n","print(response)"]},{"cell_type":"code","execution_count":48,"id":"352ad320","metadata":{"height":30},"outputs":[{"name":"stdout","output_type":"stream","text":["{'prompt_tokens': 67, 'completion_tokens': 191, 'total_tokens': 258}\n"]}],"source":["print(token_dict)"]},{"cell_type":"markdown","id":"d7f65685","metadata":{},"source":["最后，我们认为 Prompt 对 AI 应用开发的革命性影响仍未得到充分重视低。在传统的监督机器学习工作流中，如果想要构建一个可以将餐厅评论分类为正面或负面的分类器，首先需要获取一大批带有标签的数据，可能需要几百个，这个过程可能需要几周，甚至一个月的时间。接着，您需要在这些数据上训练一个模型，找到一个合适的开源模型，并进行模型的调整和评估，这个阶段可能需要几天、几周，甚至几个月的时间。最后，您可能需要使用云服务来部署模型，将模型上传到云端，并让它运行起来，才能最终调用您的模型。整个过程通常需要一个团队数月时间才能完成。\n","\n","相比之下，使用基于 Prompt 的机器学习方法，当您有一个文本应用时，只需提供一个简单的 Prompt 就可以了。这个过程可能只需要几分钟，如果需要多次迭代来得到有效的 Prompt 的话，最多几个小时即可完成。在几天内（尽管实际情况通常是几个小时），您就可以通过 API 调用来运行模型，并开始使用。一旦您达到了这个步骤，只需几分钟或几个小时，就可以开始调用模型进行推理。因此，以前可能需要花费六个月甚至一年时间才能构建的应用，现在只需要几分钟或几个小时，最多是几天的时间，就可以使用 Prompt 构建起来。这种方法正在极大地改变 AI 应用的快速构建方式。\n","\n","需要注意的是，这种方法适用于许多非结构化数据应用，特别是文本应用，以及越来越多的视觉应用，尽管目前的视觉技术仍在发展中。但它并不适用于结构化数据应用，也就是那些处理 Excel 电子表格中大量数值的机器学习应用。然而，对于适用于这种方法的应用，AI 组件可以被快速构建，并且正在改变整个系统的构建工作流。构建整个系统可能仍然需要几天、几周或更长时间，但至少这部分可以更快地完成。"]},{"cell_type":"markdown","id":"cfe248d6","metadata":{},"source":["下一个章中，我们将展示如何利用这些组件来评估客户服务助手的输入。\n","这将是本课程中构建在线零售商客户服务助手的更完整示例的一部分。"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":5}
